{"paragraphs":[{"text":"%md\n# Creating Dataframes\n\nSpark DF allows you to create dataframes from the following sources.\n\n- Option 1: Spark Sequence (built-in data structure)\n- Option 2: External sources\n    - Hive tables (connect to Hive metastore)\n    - Structured and semi-structured files from different file systems (e.g., HDFS, GS, S3, local FS)","user":"anonymous","dateUpdated":"2020-04-22T15:47:33+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h1>Creating Dataframes</h1>\n<p>Spark DF allows you to create dataframes from the following sources.</p>\n<ul>\n  <li>Option 1: Spark Sequence (built-in data structure)</li>\n  <li>Option 2: External sources\n    <ul>\n      <li>Hive tables (connect to Hive metastore)</li>\n      <li>Structured and semi-structured files from different file systems (e.g., HDFS, GS, S3, local FS)</li>\n    </ul>\n  </li>\n</ul>\n</div>"}]},"apps":[],"jobName":"paragraph_1587490820247_1624486410","id":"20200119-073223_398987810","dateCreated":"2020-04-21T17:40:20+0000","dateStarted":"2020-04-22T15:47:34+0000","dateFinished":"2020-04-22T15:47:36+0000","status":"FINISHED","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:7892"},{"text":"%md\n## Option 1: Spark Sequence\n\nSpark DF can convert a sequence of tuples to Spark DF.\ne.g., `Seq[(String, Double, String, String)]`\n\n- A tuple corresponds to a DF row.\n- An element in a tuple corresponds to a column to a particular row.\n\nPlease run and learn the paragraph below. Feel free to modify the code to test your queries.","user":"anonymous","dateUpdated":"2020-04-22T15:47:36+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":6,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h2>Option 1: Spark Sequence</h2>\n<p>Spark DF can convert a sequence of tuples to Spark DF.<br/>e.g., <code>Seq[(String, Double, String, String)]</code></p>\n<ul>\n  <li>A tuple corresponds to a DF row.</li>\n  <li>An element in a tuple corresponds to a column to a particular row.</li>\n</ul>\n<p>Please run and learn the paragraph below. Feel free to modify the code to test your queries.</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1587490820260_1159623517","id":"20190519-201210_1157722001","dateCreated":"2020-04-21T17:40:20+0000","dateStarted":"2020-04-22T15:47:36+0000","dateFinished":"2020-04-22T15:47:36+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:7893"},{"text":"%md\n### Scala Implicit Conversions (optional)\n\nIn short, you have to `import spark.implicits._` to convert/cast a `Seq[(String, Double, String, String)]` to a Spark `DataFrame`. (e.g. `lineTupleSeq.toDF`)\n\nThis is called implicit conversions in Scala. In this case, `spark.implicits.localSeqToDatasetHolder` creates a Dataset from a local Seq.\n\nSpark Scala Docs:\n\n- <a href=\"https://spark.apache.org/docs/2.3.0/api/scala/index.html#org.apache.spark.sql.SparkSession$implicits$@localSeqToDatasetHolder[T](s:Seq[T])(implicitevidence$7:org.apache.spark.sql.Encoder[T]):org.apache.spark.sql.DatasetHolder[T]\" target=\"_blank\">implicits.localSeqToDatasetHolder</a>\n- <a href=\"http://spark.apache.org/docs/2.3.0/api/scala/index.html#org.apache.spark.sql.DatasetHolder@toDF(colNames:String*):org.apache.spark.sql.DataFrame\" target=\"_blank\">DatasetHolder</a>","user":"anonymous","dateUpdated":"2020-04-22T15:47:37+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":6,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h3>Scala Implicit Conversions (optional)</h3>\n<p>In short, you have to <code>import spark.implicits._</code> to convert/cast a <code>Seq[(String, Double, String, String)]</code> to a Spark <code>DataFrame</code>. (e.g. <code>lineTupleSeq.toDF</code>)</p>\n<p>This is called implicit conversions in Scala. In this case, <code>spark.implicits.localSeqToDatasetHolder</code> creates a Dataset from a local Seq.</p>\n<p>Spark Scala Docs:</p>\n<ul>\n  <li>\n  <a href=\"https://spark.apache.org/docs/2.3.0/api/scala/index.html#org.apache.spark.sql.SparkSession$implicits$@localSeqToDatasetHolder[T](s:Seq[T])(implicitevidence$7:org.apache.spark.sql.Encoder[T]):org.apache.spark.sql.DatasetHolder[T]\" target=\"_blank\">implicits.localSeqToDatasetHolder</a></li>\n  <li>\n  <a href=\"http://spark.apache.org/docs/2.3.0/api/scala/index.html#org.apache.spark.sql.DatasetHolder@toDF(colNames:String*):org.apache.spark.sql.DataFrame\" target=\"_blank\">DatasetHolder</a></li>\n</ul>\n</div>"}]},"apps":[],"jobName":"paragraph_1587490820261_-1638408951","id":"20190520-102917_1809142825","dateCreated":"2020-04-21T17:40:20+0000","dateStarted":"2020-04-22T15:47:37+0000","dateFinished":"2020-04-22T15:47:37+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:7894"},{"text":"/**\n * Each line/record/row must be a Tuple\n * e.g.  Tuple(AAPL,110.5,2018-02-01,Apple)\n * \n * Lines are grouped into a Seq\n * List(\n *   (AAPL,110.5,2018-02-01,Apple),\n *   (AMZN,1500.52,2018-02-01,Ammazon.com),\n *   (FB,170.01,2018-02-01,Facebook)\n * )\n */\nval lineTuple1 = (\"AAPL\",110.5,\"2018-02-01\",\"Apple\")\nval lineTuple2 = (\"AMZN\",1500.52,\"2018-02-01\",\"Ammazon.com\")\nval lineTuple3 = (\"FB\",170.01,\"2018-02-01\",\"Facebook\")\nval lineTupleSeq = Seq(lineTuple1,lineTuple2,lineTuple3)\n\n//To use toDF, you must import this (see next section for details)\n//In fact Zeppellin interpreter already imported this for you\nimport spark.implicits._\nval stockDf = lineTupleSeq.toDF(\"ticker\",\"price\", \"date\", \"companyName\")\nprintln(\">>Print Schema\")\nstockDf.printSchema\n\n// SELECT * FROM stock LIMIT 3\nprintln(\">>Print 3 rows\")\nstockDf.show(3)\n\n//SELECT companyName AS company_name, price FROM stock\nprintln(\">>Print all rows with renamed columns\")\nstockDf.select(col(\"companyName\").as(\"company_name\"), col(\"price\")).show()\n\n//Use SQL to query dataframe\n//creating a tmep view\nstockDf.createOrReplaceTempView(\"stock\")\n\n//execute query\nval filteredDf = spark.sql(\"SELECT * FROM stock WHERE price > 100.0\")\nprintln(\">>SQL\")\nfilteredDf.show()\n\n","user":"anonymous","dateUpdated":"2020-04-22T15:47:37+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":">>Print Schema\nroot\n |-- ticker: string (nullable = true)\n |-- price: double (nullable = false)\n |-- date: string (nullable = true)\n |-- companyName: string (nullable = true)\n\n>>Print 3 rows\n+------+-------+----------+-----------+\n|ticker|  price|      date|companyName|\n+------+-------+----------+-----------+\n|  AAPL|  110.5|2018-02-01|      Apple|\n|  AMZN|1500.52|2018-02-01|Ammazon.com|\n|    FB| 170.01|2018-02-01|   Facebook|\n+------+-------+----------+-----------+\n\n>>Print all rows with renamed columns\n+------------+-------+\n|company_name|  price|\n+------------+-------+\n|       Apple|  110.5|\n| Ammazon.com|1500.52|\n|    Facebook| 170.01|\n+------------+-------+\n\n>>SQL\n+------+-------+----------+-----------+\n|ticker|  price|      date|companyName|\n+------+-------+----------+-----------+\n|  AAPL|  110.5|2018-02-01|      Apple|\n|  AMZN|1500.52|2018-02-01|Ammazon.com|\n|    FB| 170.01|2018-02-01|   Facebook|\n+------+-------+----------+-----------+\n\nlineTuple1: (String, Double, String, String) = (AAPL,110.5,2018-02-01,Apple)\nlineTuple2: (String, Double, String, String) = (AMZN,1500.52,2018-02-01,Ammazon.com)\nlineTuple3: (String, Double, String, String) = (FB,170.01,2018-02-01,Facebook)\nlineTupleSeq: Seq[(String, Double, String, String)] = List((AAPL,110.5,2018-02-01,Apple), (AMZN,1500.52,2018-02-01,Ammazon.com), (FB,170.01,2018-02-01,Facebook))\nimport spark.implicits._\nstockDf: org.apache.spark.sql.DataFrame = [ticker: string, price: double ... 2 more fields]\nfilteredDf: org.apache.spark.sql.DataFrame = [ticker: string, price: double ... 2 more fields]\n"}]},"apps":[],"jobName":"paragraph_1587490820263_1182630915","id":"20190519-201416_412351679","dateCreated":"2020-04-21T17:40:20+0000","dateStarted":"2020-04-22T15:47:38+0000","dateFinished":"2020-04-22T15:47:40+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:7895"},{"text":"%md\n## Option 2: External sources\nThe most common way to create Spark DFs is to read data/files from external sources. Spark has built-in features to parse CSV, JSON, and many other semistructured and structured files.\n\nPlease run and learn the paragraph below. Feel free to modify the code to test your queries.","user":"anonymous","dateUpdated":"2020-04-22T15:47:40+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h2>Option 2: External sources</h2>\n<p>The most common way to create Spark DFs is to read data/files from external sources. Spark has built-in features to parse CSV, JSON, and many other semistructured and structured files.</p>\n<p>Please run and learn the paragraph below. Feel free to modify the code to test your queries.</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1587490820264_-1391901747","id":"20190520-104920_1833330750","dateCreated":"2020-04-21T17:40:20+0000","dateStarted":"2020-04-22T15:47:40+0000","dateFinished":"2020-04-22T15:47:40+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:7896"},{"text":"//Read CSV file to df\n//local or hdfs path\nval path = \"hdfs:///user/aayushi/datasets/online_retail/online-retail-dataset.txt\"\n\n//spark.read is able to handle csv formats\nval retailDf = spark.read.format(\"csv\")\n    .option(\"header\", \"true\")\n    .option(\"inferSchema\", \"true\")\n    .load(path)\n\nretailDf.printSchema\nretailDf.show(3)\nretailDf.show(3,false)\n","user":"anonymous","dateUpdated":"2020-04-22T15:47:41+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"root\n |-- InvoiceNo: string (nullable = true)\n |-- StockCode: string (nullable = true)\n |-- Description: string (nullable = true)\n |-- Quantity: integer (nullable = true)\n |-- InvoiceDate: string (nullable = true)\n |-- UnitPrice: double (nullable = true)\n |-- CustomerID: integer (nullable = true)\n |-- Country: string (nullable = true)\n\n+---------+---------+--------------------+--------+--------------+---------+----------+--------------+\n|InvoiceNo|StockCode|         Description|Quantity|   InvoiceDate|UnitPrice|CustomerID|       Country|\n+---------+---------+--------------------+--------+--------------+---------+----------+--------------+\n|   536365|   85123A|WHITE HANGING HEA...|       6|12/1/2010 8:26|     2.55|     17850|United Kingdom|\n|   536365|    71053| WHITE METAL LANTERN|       6|12/1/2010 8:26|     3.39|     17850|United Kingdom|\n|   536365|   84406B|CREAM CUPID HEART...|       8|12/1/2010 8:26|     2.75|     17850|United Kingdom|\n+---------+---------+--------------------+--------+--------------+---------+----------+--------------+\nonly showing top 3 rows\n\n+---------+---------+----------------------------------+--------+--------------+---------+----------+--------------+\n|InvoiceNo|StockCode|Description                       |Quantity|InvoiceDate   |UnitPrice|CustomerID|Country       |\n+---------+---------+----------------------------------+--------+--------------+---------+----------+--------------+\n|536365   |85123A   |WHITE HANGING HEART T-LIGHT HOLDER|6       |12/1/2010 8:26|2.55     |17850     |United Kingdom|\n|536365   |71053    |WHITE METAL LANTERN               |6       |12/1/2010 8:26|3.39     |17850     |United Kingdom|\n|536365   |84406B   |CREAM CUPID HEARTS COAT HANGER    |8       |12/1/2010 8:26|2.75     |17850     |United Kingdom|\n+---------+---------+----------------------------------+--------+--------------+---------+----------+--------------+\nonly showing top 3 rows\n\npath: String = hdfs:///user/aayushi/datasets/online_retail/online-retail-dataset.txt\nretailDf: org.apache.spark.sql.DataFrame = [InvoiceNo: string, StockCode: string ... 6 more fields]\n"}]},"apps":[],"jobName":"paragraph_1587490820265_-2091389193","id":"20190520-095229_630927102","dateCreated":"2020-04-21T17:40:20+0000","dateStarted":"2020-04-22T15:47:41+0000","dateFinished":"2020-04-22T15:47:44+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:7897"},{"text":"%md\n### Column Type Cast\nIn the previous paragraph, the data type of the `InvoiceDate` column is String instead of `timestamp`. In this practice, you need to cast `InvoiceDate` column to Spark `timestamp` data type.\n\n```bash\nresultDf.printSchema\nroot\n |-- InvoiceNo: string (nullable = true)\n |-- StockCode: string (nullable = true)\n |-- Description: string (nullable = true)\n |-- Quantity: integer (nullable = true)\n |-- InvoiceDate: timestamp (nullable = true) #cast string to timestamp\n |-- UnitPrice: double (nullable = true)\n |-- CustomerID: integer (nullable = true)\n |-- Country: string (nullable = true)\n```\n","user":"anonymous","dateUpdated":"2020-04-22T15:47:44+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":6,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h3>Column Type Cast</h3>\n<p>In the previous paragraph, the data type of the <code>InvoiceDate</code> column is String instead of <code>timestamp</code>. In this practice, you need to cast <code>InvoiceDate</code> column to Spark <code>timestamp</code> data type.</p>\n<pre><code class=\"bash\">resultDf.printSchema\nroot\n |-- InvoiceNo: string (nullable = true)\n |-- StockCode: string (nullable = true)\n |-- Description: string (nullable = true)\n |-- Quantity: integer (nullable = true)\n |-- InvoiceDate: timestamp (nullable = true) #cast string to timestamp\n |-- UnitPrice: double (nullable = true)\n |-- CustomerID: integer (nullable = true)\n |-- Country: string (nullable = true)\n</code></pre>\n</div>"}]},"apps":[],"jobName":"paragraph_1587490820266_-1077907186","id":"20190520-085947_2007764287","dateCreated":"2020-04-21T17:40:20+0000","dateStarted":"2020-04-22T15:47:44+0000","dateFinished":"2020-04-22T15:47:44+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:7898"},{"text":"//write code to cast InvoiceData from String to timestamp\n//save the result DF as `val restaulCastDf`\n//please see sample result at the end of this paragraph (below the editor)\nval retailCastDf = retailDf.withColumn(\"InvoiceDate\", to_timestamp(col(\"InvoiceDate\"), \"MM/dd/yyy HH:mm\"))\n\n//print schema\nretailCastDf.printSchema\n//print rows to verify\nretailCastDf.show(3, false)\n//Cache DF in memory since it will be accessed frequently\nretailCastDf.cache","user":"anonymous","dateUpdated":"2020-04-22T15:47:44+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":6,"editorMode":"ace/mode/scala","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"root\n |-- InvoiceNo: string (nullable = true)\n |-- StockCode: string (nullable = true)\n |-- Description: string (nullable = true)\n |-- Quantity: integer (nullable = true)\n |-- InvoiceDate: timestamp (nullable = true)\n |-- UnitPrice: double (nullable = true)\n |-- CustomerID: integer (nullable = true)\n |-- Country: string (nullable = true)\n\n+---------+---------+----------------------------------+--------+-------------------+---------+----------+--------------+\n|InvoiceNo|StockCode|Description                       |Quantity|InvoiceDate        |UnitPrice|CustomerID|Country       |\n+---------+---------+----------------------------------+--------+-------------------+---------+----------+--------------+\n|536365   |85123A   |WHITE HANGING HEART T-LIGHT HOLDER|6       |2010-12-01 08:26:00|2.55     |17850     |United Kingdom|\n|536365   |71053    |WHITE METAL LANTERN               |6       |2010-12-01 08:26:00|3.39     |17850     |United Kingdom|\n|536365   |84406B   |CREAM CUPID HEARTS COAT HANGER    |8       |2010-12-01 08:26:00|2.75     |17850     |United Kingdom|\n+---------+---------+----------------------------------+--------+-------------------+---------+----------+--------------+\nonly showing top 3 rows\n\nretailCastDf: org.apache.spark.sql.DataFrame = [InvoiceNo: string, StockCode: string ... 6 more fields]\nres26: retailCastDf.type = [InvoiceNo: string, StockCode: string ... 6 more fields]\n"}]},"apps":[],"jobName":"paragraph_1587490820267_1620456994","id":"20190519-215300_721200493","dateCreated":"2020-04-21T17:40:20+0000","dateStarted":"2020-04-22T15:47:47+0000","dateFinished":"2020-04-22T15:47:47+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:7899"},{"text":"%md\n# DF Operations","user":"anonymous","dateUpdated":"2020-04-22T15:47:47+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h1>DF Operations</h1>\n</div>"}]},"apps":[],"jobName":"paragraph_1587490820267_-219747798","id":"20191010-103454_2098588366","dateCreated":"2020-04-21T17:40:20+0000","dateStarted":"2020-04-22T15:47:47+0000","dateFinished":"2020-04-22T15:47:47+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:7900"},{"text":"%md\n## DataFrame SELECT\nImplement the following SQL queries using dataframe. Compare different select syntax.\n\n```sql\nSELECT *\nFROM retail\nLIMIT 3\n\nSELECT InvoiceNo\nFROM retail\n\nSELECT InvoiceNo as invoiceNo\nFROM retail\n\nSELECT max(UnitPrice) as max_unit_price\nFROM retail\n```","user":"anonymous","dateUpdated":"2020-04-22T15:47:47+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":6,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h2>DataFrame SELECT</h2>\n<p>Implement the following SQL queries using dataframe. Compare different select syntax.</p>\n<pre><code class=\"sql\">SELECT *\nFROM retail\nLIMIT 3\n\nSELECT InvoiceNo\nFROM retail\n\nSELECT InvoiceNo as invoiceNo\nFROM retail\n\nSELECT max(UnitPrice) as max_unit_price\nFROM retail\n</code></pre>\n</div>"}]},"apps":[],"jobName":"paragraph_1587490820268_-909667509","id":"20190519-221054_1925024171","dateCreated":"2020-04-21T17:40:20+0000","dateStarted":"2020-04-22T15:47:48+0000","dateFinished":"2020-04-22T15:47:48+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:7901"},{"text":"//SELECT * from retail limit 1;\nretailCastDf.show(1)\nimport org.apache.spark.sql.functions._\n\n//Select * from retail limit 3;\nretailCastDf.show(3)\n\n//select InvoiceNo,CustomerID,Country from retail limit 1;\nretailCastDf.select(\"InvoiceNo\").show(1)\n\n//Select InvoiceNo from retail\nretailCastDf.select(\"InvoiceNo\").show(1)\n\n//Different ways of select \n// retailCastDf.select($\"InvoiceNo\").show(1)\n// retailCastDf.select('InvoiceNo).show(1)\n// retailCastDf.select(col(\"InvoiceNo\")).show(1)\n// retailCastDf.select(retailCastDf.col(\"InvoiceNo\")).show(1)\n// retailCastDf.select(expr(\"InvoiceNo\")).show(1)\n\n// //ERROR: cannot mix \n// //retailCastDf.select($\"InvoiceNo\", \"StockCode\").show(1)\n\n//expr or selectExpr is most powerful and close to SQL syntax\n//SELECT InvoiceNo as invoiceId from retail limit 1;\n//col(\"InvoiceNo\").as(\"invoiceId\")\nretailCastDf.select(expr(\"InvoiceNo as invoiceId\")).show(1)\nretailCastDf.selectExpr(\"InvoiceNo as invoiceId\").show(1)\n\n//SELECT * from retail limit 1;\nretailCastDf.selectExpr(\"*\").show(1)\n\n//select max(UnitPrice) as maxUnitPrice from retail\nretailCastDf.selectExpr(\"max(UnitPrice) as max_Unit_Price\").show\n","user":"anonymous","dateUpdated":"2020-04-22T15:47:48+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":6,"editorMode":"ace/mode/scala","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+\n|InvoiceNo|StockCode|         Description|Quantity|        InvoiceDate|UnitPrice|CustomerID|       Country|\n+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+\n|   536365|   85123A|WHITE HANGING HEA...|       6|2010-12-01 08:26:00|     2.55|     17850|United Kingdom|\n+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+\nonly showing top 1 row\n\n+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+\n|InvoiceNo|StockCode|         Description|Quantity|        InvoiceDate|UnitPrice|CustomerID|       Country|\n+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+\n|   536365|   85123A|WHITE HANGING HEA...|       6|2010-12-01 08:26:00|     2.55|     17850|United Kingdom|\n|   536365|    71053| WHITE METAL LANTERN|       6|2010-12-01 08:26:00|     3.39|     17850|United Kingdom|\n|   536365|   84406B|CREAM CUPID HEART...|       8|2010-12-01 08:26:00|     2.75|     17850|United Kingdom|\n+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+\nonly showing top 3 rows\n\n+---------+\n|InvoiceNo|\n+---------+\n|   536365|\n+---------+\nonly showing top 1 row\n\n+---------+\n|InvoiceNo|\n+---------+\n|   536365|\n+---------+\nonly showing top 1 row\n\n+---------+\n|invoiceId|\n+---------+\n|   536365|\n+---------+\nonly showing top 1 row\n\n+---------+\n|invoiceId|\n+---------+\n|   536365|\n+---------+\nonly showing top 1 row\n\n+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+\n|InvoiceNo|StockCode|         Description|Quantity|        InvoiceDate|UnitPrice|CustomerID|       Country|\n+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+\n|   536365|   85123A|WHITE HANGING HEA...|       6|2010-12-01 08:26:00|     2.55|     17850|United Kingdom|\n+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+\nonly showing top 1 row\n\n+--------------+\n|max_Unit_Price|\n+--------------+\n|       38970.0|\n+--------------+\n\nimport org.apache.spark.sql.functions._\n"}]},"apps":[],"jobName":"paragraph_1587490820271_-1862301652","id":"20190519-211701_1956303781","dateCreated":"2020-04-21T17:40:20+0000","dateStarted":"2020-04-22T15:47:48+0000","dateFinished":"2020-04-22T15:47:49+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:7902"},{"text":"%md\n## DataFrame filtering (WHERE)\n\nImplement the following SQL query\n\n```sql\nSELECT *\nFROM retail\nWHERE InvoiceNo = 536365\nLIMIT 2\n```\n\nSample results\n```\n+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+\n|InvoiceNo|StockCode|         Description|Quantity|        InvoiceDate|UnitPrice|CustomerID|       Country|\n+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+\n|   536365|   85123A|WHITE HANGING HEA...|       6|2010-12-01 08:26:00|     2.55|     17850|United Kingdom|\n|   536365|    71053| WHITE METAL LANTERN|       6|2010-12-01 08:26:00|     3.39|     17850|United Kingdom|\n+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+\nonly showing top 2 rows\n```","user":"anonymous","dateUpdated":"2020-04-22T15:47:49+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":6,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h2>DataFrame filtering (WHERE)</h2>\n<p>Implement the following SQL query</p>\n<pre><code class=\"sql\">SELECT *\nFROM retail\nWHERE InvoiceNo = 536365\nLIMIT 2\n</code></pre>\n<p>Sample results</p>\n<pre><code>+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+\n|InvoiceNo|StockCode|         Description|Quantity|        InvoiceDate|UnitPrice|CustomerID|       Country|\n+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+\n|   536365|   85123A|WHITE HANGING HEA...|       6|2010-12-01 08:26:00|     2.55|     17850|United Kingdom|\n|   536365|    71053| WHITE METAL LANTERN|       6|2010-12-01 08:26:00|     3.39|     17850|United Kingdom|\n+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+\nonly showing top 2 rows\n</code></pre>\n</div>"}]},"apps":[],"jobName":"paragraph_1587490820271_-1001714217","id":"20190519-221114_648626738","dateCreated":"2020-04-21T17:40:20+0000","dateStarted":"2020-04-22T15:47:50+0000","dateFinished":"2020-04-22T15:47:50+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:7903"},{"text":"//write your code here and print result\nretailCastDf.where('InvoiceNo === 536365).show(2, false)\n","user":"anonymous","dateUpdated":"2020-04-22T15:47:50+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":6,"editorMode":"ace/mode/scala","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"+---------+---------+----------------------------------+--------+-------------------+---------+----------+--------------+\n|InvoiceNo|StockCode|Description                       |Quantity|InvoiceDate        |UnitPrice|CustomerID|Country       |\n+---------+---------+----------------------------------+--------+-------------------+---------+----------+--------------+\n|536365   |85123A   |WHITE HANGING HEART T-LIGHT HOLDER|6       |2010-12-01 08:26:00|2.55     |17850     |United Kingdom|\n|536365   |71053    |WHITE METAL LANTERN               |6       |2010-12-01 08:26:00|3.39     |17850     |United Kingdom|\n+---------+---------+----------------------------------+--------+-------------------+---------+----------+--------------+\nonly showing top 2 rows\n\n"}]},"apps":[],"jobName":"paragraph_1587490820272_1837711500","id":"20190519-201625_2028882244","dateCreated":"2020-04-21T17:40:20+0000","dateStarted":"2020-04-22T15:47:50+0000","dateFinished":"2020-04-22T15:47:50+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:7904"},{"user":"anonymous","dateUpdated":"2020-04-22T15:47:50+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1587490820272_-259470495","id":"20191007-145852_244125478","dateCreated":"2020-04-21T17:40:20+0000","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:7905"},{"text":"%md\n\n# DF Exercises\nIn the following phrargraphs, you will be asked to solve some bussiness question using Spark Dataframes. However, you can use Spark SQL to verify you solution. ","user":"anonymous","dateUpdated":"2020-04-22T15:47:51+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":6,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h1>DF Exercises</h1>\n<p>In the following phrargraphs, you will be asked to solve some bussiness question using Spark Dataframes. However, you can use Spark SQL to verify you solution.</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1587490820273_-647896146","id":"20190520-123428_698724288","dateCreated":"2020-04-21T17:40:20+0000","dateStarted":"2020-04-22T15:47:51+0000","dateFinished":"2020-04-22T15:47:51+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:7906"},{"text":"//Spark SQL exercies\n//register retailCastDf as `retail` view\nretailCastDf.createOrReplaceTempView(\"retail\")\n\n//execute SQL\nspark.sql(\"SELECT * FROM retail limit 10\").show","user":"anonymous","dateUpdated":"2020-04-22T15:47:51+0000","config":{"tableHide":false,"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":6,"editorMode":"ace/mode/scala","fontSize":9,"editorHide":false,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+\n|InvoiceNo|StockCode|         Description|Quantity|        InvoiceDate|UnitPrice|CustomerID|       Country|\n+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+\n|   536365|   85123A|WHITE HANGING HEA...|       6|2010-12-01 08:26:00|     2.55|     17850|United Kingdom|\n|   536365|    71053| WHITE METAL LANTERN|       6|2010-12-01 08:26:00|     3.39|     17850|United Kingdom|\n|   536365|   84406B|CREAM CUPID HEART...|       8|2010-12-01 08:26:00|     2.75|     17850|United Kingdom|\n|   536365|   84029G|KNITTED UNION FLA...|       6|2010-12-01 08:26:00|     3.39|     17850|United Kingdom|\n|   536365|   84029E|RED WOOLLY HOTTIE...|       6|2010-12-01 08:26:00|     3.39|     17850|United Kingdom|\n|   536365|    22752|SET 7 BABUSHKA NE...|       2|2010-12-01 08:26:00|     7.65|     17850|United Kingdom|\n|   536365|    21730|GLASS STAR FROSTE...|       6|2010-12-01 08:26:00|     4.25|     17850|United Kingdom|\n|   536366|    22633|HAND WARMER UNION...|       6|2010-12-01 08:28:00|     1.85|     17850|United Kingdom|\n|   536366|    22632|HAND WARMER RED P...|       6|2010-12-01 08:28:00|     1.85|     17850|United Kingdom|\n|   536367|    84879|ASSORTED COLOUR B...|      32|2010-12-01 08:34:00|     1.69|     13047|United Kingdom|\n+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+\n\n"}]},"apps":[],"jobName":"paragraph_1587490820274_1551546609","id":"20190520-142038_1683726413","dateCreated":"2020-04-21T17:40:20+0000","dateStarted":"2020-04-22T15:47:52+0000","dateFinished":"2020-04-22T15:47:53+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:7907"},{"text":"%md\n#### Q1: Find the top N largest invoices by the amount (`Quantity * UnitPrice`)\n\nNote: `InvoiceNo` will appear in multiple rows. <br>(e.g. a receipt can have multiple items on it.)\n\n**Sample output**\n```bash\n+---------+------------------+\n|InvoiceNo|            Amount|\n+---------+------------------+\n|   581483|          168469.6|\n|   541431|           77183.6|\n|   574941| 52940.93999999999|\n|   576365|50653.909999999996|\n|   556444|           38970.0|\n+---------+------------------+\n```","user":"anonymous","dateUpdated":"2020-04-22T15:47:53+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":6,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h4>Q1: Find the top N largest invoices by the amount (<code>Quantity * UnitPrice</code>)</h4>\n<p>Note: <code>InvoiceNo</code> will appear in multiple rows. <br>(e.g. a receipt can have multiple items on it.)</p>\n<p><strong>Sample output</strong></p>\n<pre><code class=\"bash\">+---------+------------------+\n|InvoiceNo|            Amount|\n+---------+------------------+\n|   581483|          168469.6|\n|   541431|           77183.6|\n|   574941| 52940.93999999999|\n|   576365|50653.909999999996|\n|   556444|           38970.0|\n+---------+------------------+\n</code></pre>\n</div>"}]},"apps":[],"jobName":"paragraph_1587490820275_219375429","id":"20190520-133812_405266917","dateCreated":"2020-04-21T17:40:20+0000","dateStarted":"2020-04-22T15:47:53+0000","dateFinished":"2020-04-22T15:47:53+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:7908"},{"text":"//write you DF solution here\n\nval largestAmountInvoice = retailCastDf.groupBy(col(\"InvoiceNo\")).agg(sum(col(\"Quantity\") * col(\"UnitPrice\")) as \"Amount\").orderBy('Amount.desc)\nlargestAmountInvoice.show(5)\n\n//please verify using SparkSQL\nspark.sql(\"SELECT InvoiceNo, SUM(Quantity * UnitPrice) AS Amount FROM retail GROUP BY InvoiceNo ORDER BY Amount DESC LIMIT 5\").show(5)","user":"anonymous","dateUpdated":"2020-04-22T15:47:53+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":6,"editorMode":"ace/mode/scala","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"+---------+------------------+\n|InvoiceNo|            Amount|\n+---------+------------------+\n|   581483|          168469.6|\n|   541431|           77183.6|\n|   574941| 52940.93999999999|\n|   576365|50653.909999999996|\n|   556444|           38970.0|\n+---------+------------------+\nonly showing top 5 rows\n\n+---------+------------------+\n|InvoiceNo|            Amount|\n+---------+------------------+\n|   581483|          168469.6|\n|   541431|           77183.6|\n|   574941| 52940.93999999999|\n|   576365|50653.909999999996|\n|   556444|           38970.0|\n+---------+------------------+\n\nlargestAmountInvoice: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [InvoiceNo: string, Amount: double]\n"}]},"apps":[],"jobName":"paragraph_1587490820275_-725815136","id":"20190519-215312_1016690251","dateCreated":"2020-04-21T17:40:20+0000","dateStarted":"2020-04-22T15:47:53+0000","dateFinished":"2020-04-22T15:47:57+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:7909"},{"text":"retailDf.columns","user":"anonymous","dateUpdated":"2020-04-22T15:47:57+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"res31: Array[String] = Array(InvoiceNo, StockCode, Description, Quantity, InvoiceDate, UnitPrice, CustomerID, Country)\n"}]},"apps":[],"jobName":"paragraph_1587490820276_315223775","id":"20191007-145909_914572499","dateCreated":"2020-04-21T17:40:20+0000","dateStarted":"2020-04-22T15:47:57+0000","dateFinished":"2020-04-22T15:47:57+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:7910"},{"text":"%md\n#### Q2: Find the top N largest invoices by the amount and show receipt details\n\n```\n+---------+------------------+-------------------+----------+--------------+\n|InvoiceNo|            Amount|        InvoiceDate|CustomerID|       Country|\n+---------+------------------+-------------------+----------+--------------+\n|   581483|          168469.6|2011-12-09 09:15:00|     16446|United Kingdom|\n|   541431|           77183.6|2011-01-18 10:01:00|     12346|United Kingdom|\n|   574941| 52940.93999999999|2011-11-07 17:42:00|      null|United Kingdom|\n|   576365|50653.909999999996|2011-11-14 17:55:00|      null|United Kingdom|\n|   556444|           38970.0|2011-06-10 15:28:00|     15098|United Kingdom|\n+---------+------------------+-------------------+----------+--------------+\n```","user":"anonymous","dateUpdated":"2020-04-22T15:47:57+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":6,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h4>Q2: Find the top N largest invoices by the amount and show receipt details</h4>\n<pre><code>+---------+------------------+-------------------+----------+--------------+\n|InvoiceNo|            Amount|        InvoiceDate|CustomerID|       Country|\n+---------+------------------+-------------------+----------+--------------+\n|   581483|          168469.6|2011-12-09 09:15:00|     16446|United Kingdom|\n|   541431|           77183.6|2011-01-18 10:01:00|     12346|United Kingdom|\n|   574941| 52940.93999999999|2011-11-07 17:42:00|      null|United Kingdom|\n|   576365|50653.909999999996|2011-11-14 17:55:00|      null|United Kingdom|\n|   556444|           38970.0|2011-06-10 15:28:00|     15098|United Kingdom|\n+---------+------------------+-------------------+----------+--------------+\n</code></pre>\n</div>"}]},"apps":[],"jobName":"paragraph_1587490820277_522287464","id":"20190520-124355_215736883","dateCreated":"2020-04-21T17:40:20+0000","dateStarted":"2020-04-22T15:47:57+0000","dateFinished":"2020-04-22T15:47:58+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:7911"},{"text":"//write your DF solution here\n\nval receiptAmountDf = retailCastDf.groupBy('InvoiceNo, 'InvoiceDate, 'CustomerID, 'Country).agg(sum($\"Quantity\" * $\"UnitPrice\").alias(\"Amount\")).orderBy('Amount.desc)\nreceiptAmountDf.show(5)\nreceiptAmountDf.createOrReplaceTempView(\"invoiceView\")\n\n//please verify using SparkSQL\nspark.sql(\"SELECT largeAmountDf.InvoiceNo, largeAmountDf.Amount, retailCastDf.InvoiceDate, retailCastDf.CustomerID, retailCastDf.Country FROM (SELECT InvoiceNo, SUM(Quantity * UnitPrice) AS Amount FROM retail GROUP BY InvoiceNo) AS largeAmountDf INNER JOIN (SELECT DISTINCT(InvoiceNo), InvoiceDate, CustomerID, Country FROM retail) AS retailCastDf ON largeAmountDf.InvoiceNo=retailCastDf.InvoiceNo ORDER BY largeAmountDf.Amount DESC LIMIT 5\").show()\n\n","user":"anonymous","dateUpdated":"2020-04-24T14:36:06+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":6,"editorMode":"ace/mode/scala","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"+---------+-------------------+----------+--------------+------------------+\n|InvoiceNo|        InvoiceDate|CustomerID|       Country|            Amount|\n+---------+-------------------+----------+--------------+------------------+\n|   581483|2011-12-09 09:15:00|     16446|United Kingdom|          168469.6|\n|   541431|2011-01-18 10:01:00|     12346|United Kingdom|           77183.6|\n|   574941|2011-11-07 17:42:00|      null|United Kingdom| 52940.93999999999|\n|   576365|2011-11-14 17:55:00|      null|United Kingdom|50653.909999999996|\n|   556444|2011-06-10 15:28:00|     15098|United Kingdom|           38970.0|\n+---------+-------------------+----------+--------------+------------------+\nonly showing top 5 rows\n\n+---------+------------------+-------------------+----------+--------------+\n|InvoiceNo|            Amount|        InvoiceDate|CustomerID|       Country|\n+---------+------------------+-------------------+----------+--------------+\n|   581483|          168469.6|2011-12-09 09:15:00|     16446|United Kingdom|\n|   541431|           77183.6|2011-01-18 10:01:00|     12346|United Kingdom|\n|   574941| 52940.93999999999|2011-11-07 17:42:00|      null|United Kingdom|\n|   576365|50653.909999999996|2011-11-14 17:55:00|      null|United Kingdom|\n|   556444|           38970.0|2011-06-10 15:28:00|     15098|United Kingdom|\n+---------+------------------+-------------------+----------+--------------+\n\nreceiptAmountDf: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [InvoiceNo: string, InvoiceDate: timestamp ... 3 more fields]\n"}]},"apps":[],"jobName":"paragraph_1587490820278_-1626912097","id":"20190520-122626_1736024345","dateCreated":"2020-04-21T17:40:20+0000","dateStarted":"2020-04-22T16:26:33+0000","dateFinished":"2020-04-22T16:26:45+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:7912"},{"text":"%md\n#### Q3: For each country, find the top N largest invoices by the amount and show receipt details\n\nUse `Window functions` and `rank()` function\n\nReadings:\n- https://databricks.com/blog/2015/07/15/introducing-window-functions-in-spark-sql.html\n- https://stackoverflow.com/questions/42966590/how-do-we-rank-dataframe\n- http://blog.madhukaraphatak.com/introduction-to-spark-two-part-5/\n- `Spark The Definitive Guide - page 134 - Windows Function`\n\n```\n+---------+------------------+-------------------+----------+---------+\n|InvoiceNo|            amount|        InvoiceDate|CustomerID|  Country|\n+---------+------------------+-------------------+----------+---------+\n|   571318| 5296.960000000001|2011-10-17 10:50:00|     17404|   Sweden|\n|   546530| 4400.280000000001|2011-03-14 13:25:00|     17404|   Sweden|\n|   571751|6068.0599999999995|2011-10-19 11:18:00|     12744|Singapore|\n|   548813|4037.7700000000004|2011-04-04 13:03:00|     12744|Singapore|\n|   552978| 9341.260000000004|2011-05-12 14:46:00|     12590|  Germany|\n|   564856|4257.0599999999995|2011-08-31 09:11:00|     12477|  Germany|\n|   571035|1002.3099999999998|2011-10-13 12:50:00|     12446|      RSA|\n|   573153| 8895.760000000004|2011-10-28 07:39:00|     12678|   France|\n|   570672| 4279.710000000004|2011-10-11 14:52:00|     12536|   France|\n|   541932|           2661.24|2011-01-24 11:39:00|     14439|   Greece|\n+---------+------------------+-------------------+----------+---------+\n```\n<br>\n<br>\n**Hints**:\n- At high level, you need to create a new column which indicates amount rank by country\n  - Use `Windows` function which partition by (\"Country\") and order by amount\n  - User `Rank()` function create a new `rank` column for each row\n  - filter out rows where `rank > 2`","user":"anonymous","dateUpdated":"2020-04-22T15:48:25+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h4>Q3: For each country, find the top N largest invoices by the amount and show receipt details</h4>\n<p>Use <code>Window functions</code> and <code>rank()</code> function</p>\n<p>Readings:<br/>- <a href=\"https://databricks.com/blog/2015/07/15/introducing-window-functions-in-spark-sql.html\">https://databricks.com/blog/2015/07/15/introducing-window-functions-in-spark-sql.html</a><br/>- <a href=\"https://stackoverflow.com/questions/42966590/how-do-we-rank-dataframe\">https://stackoverflow.com/questions/42966590/how-do-we-rank-dataframe</a><br/>- <a href=\"http://blog.madhukaraphatak.com/introduction-to-spark-two-part-5/\">http://blog.madhukaraphatak.com/introduction-to-spark-two-part-5/</a><br/>- <code>Spark The Definitive Guide - page 134 - Windows Function</code></p>\n<pre><code>+---------+------------------+-------------------+----------+---------+\n|InvoiceNo|            amount|        InvoiceDate|CustomerID|  Country|\n+---------+------------------+-------------------+----------+---------+\n|   571318| 5296.960000000001|2011-10-17 10:50:00|     17404|   Sweden|\n|   546530| 4400.280000000001|2011-03-14 13:25:00|     17404|   Sweden|\n|   571751|6068.0599999999995|2011-10-19 11:18:00|     12744|Singapore|\n|   548813|4037.7700000000004|2011-04-04 13:03:00|     12744|Singapore|\n|   552978| 9341.260000000004|2011-05-12 14:46:00|     12590|  Germany|\n|   564856|4257.0599999999995|2011-08-31 09:11:00|     12477|  Germany|\n|   571035|1002.3099999999998|2011-10-13 12:50:00|     12446|      RSA|\n|   573153| 8895.760000000004|2011-10-28 07:39:00|     12678|   France|\n|   570672| 4279.710000000004|2011-10-11 14:52:00|     12536|   France|\n|   541932|           2661.24|2011-01-24 11:39:00|     14439|   Greece|\n+---------+------------------+-------------------+----------+---------+\n</code></pre>\n<br>\n<br>\n<p><strong>Hints</strong>:<br/>- At high level, you need to create a new column which indicates amount rank by country<br/> - Use <code>Windows</code> function which partition by (&ldquo;Country&rdquo;) and order by amount<br/> - User <code>Rank()</code> function create a new <code>rank</code> column for each row<br/> - filter out rows where <code>rank &gt; 2</code></p>\n</div>"}]},"apps":[],"jobName":"paragraph_1587490820278_-114767626","id":"20190520-150543_915955507","dateCreated":"2020-04-21T17:40:20+0000","dateStarted":"2020-04-22T15:48:25+0000","dateFinished":"2020-04-22T15:48:25+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:7913"},{"text":"//write you DF solution here\nimport org.apache.spark.sql.expressions.Window\nval rankInvoiceDf = receiptAmountDf.withColumn(\"rank\", row_number().over(Window.partitionBy('Country).orderBy('Amount.desc)))\nrankInvoiceDf.select('InvoiceNo, 'Amount, 'InvoiceDate, 'CustomerID, 'Country).where('rank < 3).show(12)\n\n//please verify using SparkSQL\nspark.sql(\"SELECT InvoiceNo, Amount, InvoiceDate, CustomerID, Country FROM (SELECT *, RANK() OVER (partition by Country ORDER BY Amount DESC) AS rank FROM invoiceView) WHERE rank<3 LIMIT 12\").show\n","user":"anonymous","dateUpdated":"2020-04-22T16:29:13+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"+---------+------------------+-------------------+----------+------------------+\n|InvoiceNo|            Amount|        InvoiceDate|CustomerID|           Country|\n+---------+------------------+-------------------+----------+------------------+\n|   571318| 5296.960000000001|2011-10-17 10:50:00|     17404|            Sweden|\n|   546530| 4400.280000000001|2011-03-14 13:25:00|     17404|            Sweden|\n|   571751|6068.0599999999995|2011-10-19 11:18:00|     12744|         Singapore|\n|   548813|4037.7700000000004|2011-04-04 13:03:00|     12744|         Singapore|\n|   552978| 9341.260000000004|2011-05-12 14:46:00|     12590|           Germany|\n|   564856|4257.0599999999995|2011-08-31 09:11:00|     12477|           Germany|\n|   571035|1002.3099999999998|2011-10-13 12:50:00|     12446|               RSA|\n|   573153| 8895.760000000004|2011-10-28 07:39:00|     12678|            France|\n|   570672| 4279.710000000004|2011-10-11 14:52:00|     12536|            France|\n|   541932|           2661.24|2011-01-24 11:39:00|     14439|            Greece|\n|   580752| 680.9900000000002|2011-12-06 09:56:00|     12478|            Greece|\n|   560783|             676.8|2011-07-21 10:24:00|     15108|European Community|\n+---------+------------------+-------------------+----------+------------------+\nonly showing top 12 rows\n\n+---------+------------------+-------------------+----------+------------------+\n|InvoiceNo|            Amount|        InvoiceDate|CustomerID|           Country|\n+---------+------------------+-------------------+----------+------------------+\n|   571318| 5296.960000000001|2011-10-17 10:50:00|     17404|            Sweden|\n|   546530| 4400.280000000001|2011-03-14 13:25:00|     17404|            Sweden|\n|   571035|1002.3099999999998|2011-10-13 12:50:00|     12446|               RSA|\n|   573153| 8895.760000000004|2011-10-28 07:39:00|     12678|            France|\n|   570672| 4279.710000000004|2011-10-11 14:52:00|     12536|            France|\n|   560783|             676.8|2011-07-21 10:24:00|     15108|European Community|\n|   555542|            387.05|2011-06-05 12:51:00|     15108|European Community|\n|   571030|           1491.59|2011-10-13 12:26:00|     12876|           Belgium|\n|   577046|           1342.71|2011-11-17 13:46:00|     12449|           Belgium|\n|   563353|             905.5|2011-08-15 14:32:00|     15480|             Malta|\n|   555931| 833.1600000000001|2011-06-08 08:31:00|     17828|             Malta|\n|   552695| 852.6800000000002|2011-05-10 15:31:00|     16320|       Unspecified|\n+---------+------------------+-------------------+----------+------------------+\n\nimport org.apache.spark.sql.expressions.Window\nrankInvoiceDf: org.apache.spark.sql.DataFrame = [InvoiceNo: string, InvoiceDate: timestamp ... 4 more fields]\n"}]},"apps":[],"jobName":"paragraph_1587490820279_1517775823","id":"20190520-125029_1350468290","dateCreated":"2020-04-21T17:40:20+0000","dateStarted":"2020-04-22T16:29:13+0000","dateFinished":"2020-04-22T16:29:34+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:7914"},{"text":"%md\n\n#### Q4: Generate a daily and a weekly sales table and plot diagrams using Zeppelin built-in plot.\n\n\n```bash\ndailyDf.show(5)\n+-------------------+------------------+\n|              start|       sum(amount)|\n+-------------------+------------------+\n|2010-11-30 19:00:00|          58833.88|\n|2010-12-01 19:00:00| 45666.62999999999|\n|2010-12-02 19:00:00| 46161.11000000004|\n|2010-12-04 19:00:00|31383.949999999997|\n|2010-12-05 19:00:00| 53860.18000000004|\n+-------------------+------------------+\n```\n\n```bash\nweeklyDf.show(5)\n+-------------------+------------------+\n|              start|       sum(amount)|\n+-------------------+------------------+\n|2010-11-24 19:00:00| 58833.88000000002|\n|2010-12-01 19:00:00|         266320.76|\n|2010-12-08 19:00:00|234844.27999999997|\n|2010-12-15 19:00:00|177360.10999999993|\n|2010-12-22 19:00:00|11796.309999999992|\n+-------------------+------------------+\n```\n\nReadings\n- https://databricks.com/blog/2017/05/08/event-time-aggregation-watermarking-apache-sparks-structured-streaming.html\n- http://blog.madhukaraphatak.com/introduction-to-spark-two-part-5/","user":"anonymous","dateUpdated":"2020-04-22T15:48:48+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h4>Q4: Generate a daily and a weekly sales table and plot diagrams using Zeppelin built-in plot.</h4>\n<pre><code class=\"bash\">dailyDf.show(5)\n+-------------------+------------------+\n|              start|       sum(amount)|\n+-------------------+------------------+\n|2010-11-30 19:00:00|          58833.88|\n|2010-12-01 19:00:00| 45666.62999999999|\n|2010-12-02 19:00:00| 46161.11000000004|\n|2010-12-04 19:00:00|31383.949999999997|\n|2010-12-05 19:00:00| 53860.18000000004|\n+-------------------+------------------+\n</code></pre>\n<pre><code class=\"bash\">weeklyDf.show(5)\n+-------------------+------------------+\n|              start|       sum(amount)|\n+-------------------+------------------+\n|2010-11-24 19:00:00| 58833.88000000002|\n|2010-12-01 19:00:00|         266320.76|\n|2010-12-08 19:00:00|234844.27999999997|\n|2010-12-15 19:00:00|177360.10999999993|\n|2010-12-22 19:00:00|11796.309999999992|\n+-------------------+------------------+\n</code></pre>\n<p>Readings<br/>- <a href=\"https://databricks.com/blog/2017/05/08/event-time-aggregation-watermarking-apache-sparks-structured-streaming.html\">https://databricks.com/blog/2017/05/08/event-time-aggregation-watermarking-apache-sparks-structured-streaming.html</a><br/>- <a href=\"http://blog.madhukaraphatak.com/introduction-to-spark-two-part-5/\">http://blog.madhukaraphatak.com/introduction-to-spark-two-part-5/</a></p>\n</div>"}]},"apps":[],"jobName":"paragraph_1587490820280_1000635038","id":"20190520-140931_1510736707","dateCreated":"2020-04-21T17:40:20+0000","dateStarted":"2020-04-22T15:48:48+0000","dateFinished":"2020-04-22T15:48:48+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:7915"},{"text":"//Daily\n//write you DF solution here\nval dailySalesDf = receiptAmountDf.groupBy(window(col(\"InvoiceDate\"), \"1 day\").getField(\"start\").alias(\"start\")).agg(sum('Amount).as(\"dailySum\")).orderBy('start)\ndailySalesDf.show(5)\ndailySalesDf.createOrReplaceTempView(\"dailySales\")\n\n//please verify using SparkSQL\nspark.sql(\"SELECT date_trunc('day', InvoiceDate) as start, sum(Amount) as dailySum FROM invoiceView GROUP BY start ORDER BY start ASC LIMIT 5\").show\n\n","user":"anonymous","dateUpdated":"2020-04-22T17:14:10+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":6,"editorMode":"ace/mode/scala","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"+-------------------+-----------------+\n|              start|         dailySum|\n+-------------------+-----------------+\n|2010-12-01 00:00:00|58635.56000000002|\n|2010-12-02 00:00:00|46207.27999999999|\n|2010-12-03 00:00:00|45620.46000000003|\n|2010-12-05 00:00:00|         31383.95|\n|2010-12-06 00:00:00|53860.18000000002|\n+-------------------+-----------------+\nonly showing top 5 rows\n\n+-------------------+------------------+\n|              start|          dailySum|\n+-------------------+------------------+\n|2010-12-01 00:00:00| 58635.56000000001|\n|2010-12-02 00:00:00|          46207.28|\n|2010-12-03 00:00:00|45620.460000000036|\n|2010-12-05 00:00:00| 31383.95000000001|\n|2010-12-06 00:00:00|53860.180000000044|\n+-------------------+------------------+\n\ndailySalesDf: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [start: timestamp, dailySum: double]\n"}]},"apps":[],"jobName":"paragraph_1587490820281_1171852474","id":"20190520-181045_1661878813","dateCreated":"2020-04-21T17:40:20+0000","dateStarted":"2020-04-22T17:14:10+0000","dateFinished":"2020-04-22T17:14:26+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:7916"},{"text":"%sql\n-- Plot daily diagram here\nselect * from dailySales","user":"anonymous","dateUpdated":"2020-04-24T14:41:52+0000","config":{"editorSetting":{"language":"sql","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":6,"editorMode":"ace/mode/sql","fontSize":9,"results":{"0":{"graph":{"mode":"lineChart","height":326,"optionOpen":false,"setting":{"table":{"tableGridState":{},"tableColumnTypeState":{"names":{"start":"string","sum(Amount)":"string"},"updated":false},"tableOptionSpecHash":"[{\"name\":\"useFilter\",\"valueType\":\"boolean\",\"defaultValue\":false,\"widget\":\"checkbox\",\"description\":\"Enable filter for columns\"},{\"name\":\"showPagination\",\"valueType\":\"boolean\",\"defaultValue\":false,\"widget\":\"checkbox\",\"description\":\"Enable pagination for better navigation\"},{\"name\":\"showAggregationFooter\",\"valueType\":\"boolean\",\"defaultValue\":false,\"widget\":\"checkbox\",\"description\":\"Enable a footer for displaying aggregated values\"}]","tableOptionValue":{"useFilter":false,"showPagination":false,"showAggregationFooter":false},"updated":false,"initialized":false},"multiBarChart":{"rotate":{"degree":"-45"},"xLabelStatus":"default"},"lineChart":{"rotate":{"degree":"-45"},"xLabelStatus":"default","forceY":false,"lineWithFocus":false,"isDateFormat":false},"stackedAreaChart":{"rotate":{"degree":"-45"},"xLabelStatus":"default"},"pieChart":{}},"commonSetting":{},"keys":[{"name":"start","index":0,"aggr":"sum"}],"groups":[],"values":[{"name":"dailySum","index":1,"aggr":"sum"}]},"helium":{}}},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TABLE","data":"start\tdailySum\n2010-12-01 00:00:00.0\t58635.56000000005\n2010-12-02 00:00:00.0\t46207.28000000001\n2010-12-03 00:00:00.0\t45620.46000000004\n2010-12-05 00:00:00.0\t31383.95\n2010-12-06 00:00:00.0\t53860.18000000002\n2010-12-07 00:00:00.0\t45059.05000000003\n2010-12-08 00:00:00.0\t44189.84\n2010-12-09 00:00:00.0\t52532.130000000005\n2010-12-10 00:00:00.0\t57404.91\n2010-12-12 00:00:00.0\t17240.92\n2010-12-13 00:00:00.0\t35379.340000000004\n2010-12-14 00:00:00.0\t42843.28999999999\n2010-12-15 00:00:00.0\t29443.689999999995\n2010-12-16 00:00:00.0\t48334.35\n2010-12-17 00:00:00.0\t43534.19000000001\n2010-12-19 00:00:00.0\t7517.3099999999995\n2010-12-20 00:00:00.0\t24741.749999999985\n2010-12-21 00:00:00.0\t47097.939999999995\n2010-12-22 00:00:00.0\t6134.570000000001\n2010-12-23 00:00:00.0\t11796.30999999999\n2011-01-04 00:00:00.0\t14950.479999999998\n2011-01-05 00:00:00.0\t-1566.2299999999923\n2011-01-06 00:00:00.0\t37392.74\n2011-01-07 00:00:00.0\t27233.140000000007\n2011-01-09 00:00:00.0\t15710.800000000005\n2011-01-10 00:00:00.0\t24191.639999999996\n2011-01-11 00:00:00.0\t67817.12999999999\n2011-01-12 00:00:00.0\t23958.780000000017\n2011-01-13 00:00:00.0\t20533.540000000005\n2011-01-14 00:00:00.0\t47377.259999999995\n2011-01-16 00:00:00.0\t7116.61\n2011-01-17 00:00:00.0\t29256.000000000022\n2011-01-18 00:00:00.0\t18680.799999999996\n2011-01-19 00:00:00.0\t25585.81\n2011-01-20 00:00:00.0\t17995.91\n2011-01-21 00:00:00.0\t31978.439999999995\n2011-01-23 00:00:00.0\t10285.949999999999\n2011-01-24 00:00:00.0\t25555.620000000003\n2011-01-25 00:00:00.0\t27971.520000000008\n2011-01-26 00:00:00.0\t19493.319999999996\n2011-01-27 00:00:00.0\t21092.14\n2011-01-28 00:00:00.0\t18567.769999999997\n2011-01-30 00:00:00.0\t6456.44\n2011-01-31 00:00:00.0\t22364.649999999987\n2011-02-01 00:00:00.0\t28433.22\n2011-02-02 00:00:00.0\t21048.45\n2011-02-03 00:00:00.0\t23344.58\n2011-02-04 00:00:00.0\t24994.169999999995\n2011-02-06 00:00:00.0\t3457.11\n2011-02-07 00:00:00.0\t25525.99\n2011-02-08 00:00:00.0\t20728.140000000003\n2011-02-09 00:00:00.0\t16692.58\n2011-02-10 00:00:00.0\t13427.540000000006\n2011-02-11 00:00:00.0\t20387.280000000002\n2011-02-13 00:00:00.0\t5535.400000000001\n2011-02-14 00:00:00.0\t26222.030000000002\n2011-02-15 00:00:00.0\t36842.579999999994\n2011-02-16 00:00:00.0\t24730.80999999999\n2011-02-17 00:00:00.0\t26361.870000000006\n2011-02-18 00:00:00.0\t15928.399999999998\n2011-02-20 00:00:00.0\t9578.89\n2011-02-21 00:00:00.0\t23807.83000000001\n2011-02-22 00:00:00.0\t32292.62\n2011-02-23 00:00:00.0\t26792.76000000001\n2011-02-24 00:00:00.0\t22655.83\n2011-02-25 00:00:00.0\t18029.840000000004\n2011-02-27 00:00:00.0\t9491.050000000001\n2011-02-28 00:00:00.0\t21753.680000000015\n2011-03-01 00:00:00.0\t25471.709999999992\n2011-03-02 00:00:00.0\t18296.449999999997\n2011-03-03 00:00:00.0\t35842.62\n2011-03-04 00:00:00.0\t19474.870000000006\n2011-03-06 00:00:00.0\t9596.230000000003\n2011-03-07 00:00:00.0\t30525.579999999998\n2011-03-08 00:00:00.0\t25017.47\n2011-03-09 00:00:00.0\t21907.120000000006\n2011-03-10 00:00:00.0\t25597.889999999996\n2011-03-11 00:00:00.0\t21995.280000000006\n2011-03-13 00:00:00.0\t4137.62\n2011-03-14 00:00:00.0\t25864.589999999997\n2011-03-15 00:00:00.0\t20660.030000000002\n2011-03-16 00:00:00.0\t21182.64\n2011-03-17 00:00:00.0\t38804.24999999999\n2011-03-18 00:00:00.0\t16770.459999999992\n2011-03-20 00:00:00.0\t21980.640000000003\n2011-03-21 00:00:00.0\t16370.270000000008\n2011-03-22 00:00:00.0\t31312.350000000002\n2011-03-23 00:00:00.0\t24029.07000000001\n2011-03-24 00:00:00.0\t36562.1\n2011-03-25 00:00:00.0\t30656.030000000002\n2011-03-27 00:00:00.0\t8979.980000000001\n2011-03-28 00:00:00.0\t19207.029999999995\n2011-03-29 00:00:00.0\t70531.47\n2011-03-30 00:00:00.0\t31489.25\n2011-03-31 00:00:00.0\t31004.08000000001\n2011-04-01 00:00:00.0\t24391.78000000001\n2011-04-03 00:00:00.0\t6878.1\n2011-04-04 00:00:00.0\t25073.02\n2011-04-05 00:00:00.0\t28353.83000000001\n2011-04-06 00:00:00.0\t17279.35\n2011-04-07 00:00:00.0\t18228.999999999996\n2011-04-08 00:00:00.0\t23299.139999999992\n2011-04-10 00:00:00.0\t9363.88\n2011-04-11 00:00:00.0\t22110.309999999998\n2011-04-12 00:00:00.0\t25124.249999999996\n2011-04-13 00:00:00.0\t23898.2\n2011-04-14 00:00:00.0\t35295.58\n2011-04-15 00:00:00.0\t28327.131\n2011-04-17 00:00:00.0\t12704.3\n2011-04-18 00:00:00.0\t32185.610000000004\n2011-04-19 00:00:00.0\t23837.650000000012\n2011-04-20 00:00:00.0\t28239.390000000003\n2011-04-21 00:00:00.0\t31198.600000000006\n2011-04-26 00:00:00.0\t30585.539999999994\n2011-04-27 00:00:00.0\t25590.56000000001\n2011-04-28 00:00:00.0\t21241.900000000005\n2011-05-01 00:00:00.0\t6964.660000000001\n2011-05-03 00:00:00.0\t19617.86\n2011-05-04 00:00:00.0\t27462.300000000007\n2011-05-05 00:00:00.0\t28750.649999999994\n2011-05-06 00:00:00.0\t35714.58\n2011-05-08 00:00:00.0\t18808.920000000002\n2011-05-09 00:00:00.0\t26060.43000000001\n2011-05-10 00:00:00.0\t45564.120000000024\n2011-05-11 00:00:00.0\t33240.36000000001\n2011-05-12 00:00:00.0\t59911.97\n2011-05-13 00:00:00.0\t30744.07\n2011-05-15 00:00:00.0\t9924.28\n2011-05-16 00:00:00.0\t25279.770000000008\n2011-05-17 00:00:00.0\t53603.82999999999\n2011-05-18 00:00:00.0\t34337.28999999999\n2011-05-19 00:00:00.0\t34348.75\n2011-05-20 00:00:00.0\t26256.519999999993\n2011-05-22 00:00:00.0\t24205.370000000003\n2011-05-23 00:00:00.0\t30739.55\n2011-05-24 00:00:00.0\t37028.91000000001\n2011-05-25 00:00:00.0\t24152.28\n2011-05-26 00:00:00.0\t33208.590000000004\n2011-05-27 00:00:00.0\t28232.190000000002\n2011-05-29 00:00:00.0\t7208.3\n2011-05-31 00:00:00.0\t21967.960000000003\n2011-06-01 00:00:00.0\t20191.200000000008\n2011-06-02 00:00:00.0\t32502.009999999995\n2011-06-03 00:00:00.0\t16750.999999999996\n2011-06-05 00:00:00.0\t25520.35\n2011-06-06 00:00:00.0\t16791.39\n2011-06-07 00:00:00.0\t37644.30000000001\n2011-06-08 00:00:00.0\t42940.910000000025\n2011-06-09 00:00:00.0\t45515.749999999985\n2011-06-10 00:00:00.0\t22540.660000000003\n2011-06-12 00:00:00.0\t12483.86\n2011-06-13 00:00:00.0\t20372.929999999986\n2011-06-14 00:00:00.0\t40211.93000000002\n2011-06-15 00:00:00.0\t46139.17999999999\n2011-06-16 00:00:00.0\t34131.729999999996\n2011-06-17 00:00:00.0\t20800.719999999998\n2011-06-19 00:00:00.0\t22360.01000000001\n2011-06-20 00:00:00.0\t33493.399999999994\n2011-06-21 00:00:00.0\t22730.01\n2011-06-22 00:00:00.0\t21794.940000000002\n2011-06-23 00:00:00.0\t24273.309999999998\n2011-06-24 00:00:00.0\t8619.88\n2011-06-26 00:00:00.0\t6175.169999999997\n2011-06-27 00:00:00.0\t16823.86\n2011-06-28 00:00:00.0\t34704.63999999999\n2011-06-29 00:00:00.0\t21775.430000000004\n2011-06-30 00:00:00.0\t43834.549999999996\n2011-07-01 00:00:00.0\t13171.82\n2011-07-03 00:00:00.0\t5977.14\n2011-07-04 00:00:00.0\t44154.75000000001\n2011-07-05 00:00:00.0\t40334.97000000001\n2011-07-06 00:00:00.0\t26279.58\n2011-07-07 00:00:00.0\t31357.720000000005\n2011-07-08 00:00:00.0\t26840.080000000005\n2011-07-10 00:00:00.0\t5692.07\n2011-07-11 00:00:00.0\t22429.53\n2011-07-12 00:00:00.0\t25892.040000000008\n2011-07-13 00:00:00.0\t11612.050000000012\n2011-07-14 00:00:00.0\t32575.96000000001\n2011-07-15 00:00:00.0\t14478.930000000008\n2011-07-17 00:00:00.0\t17174.659999999996\n2011-07-18 00:00:00.0\t28443.27000000001\n2011-07-19 00:00:00.0\t49316.78000000002\n2011-07-20 00:00:00.0\t27305.410000000014\n2011-07-21 00:00:00.0\t30957.069999999992\n2011-07-22 00:00:00.0\t20015.23000000001\n2011-07-24 00:00:00.0\t26476.19999999999\n2011-07-25 00:00:00.0\t26687.650000000005\n2011-07-26 00:00:00.0\t21271.300999999996\n2011-07-27 00:00:00.0\t25568.450000000004\n2011-07-28 00:00:00.0\t55706.88000000003\n2011-07-29 00:00:00.0\t18094.209999999995\n2011-07-31 00:00:00.0\t33486.36000000001\n2011-08-01 00:00:00.0\t21362.84\n2011-08-02 00:00:00.0\t14947.270000000006\n2011-08-03 00:00:00.0\t27075.020000000015\n2011-08-04 00:00:00.0\t61028.64999999998\n2011-08-05 00:00:00.0\t21298.299999999996\n2011-08-07 00:00:00.0\t7464.12\n2011-08-08 00:00:00.0\t19987.150000000005\n2011-08-09 00:00:00.0\t26623.199999999993\n2011-08-10 00:00:00.0\t27474.21999999999\n2011-08-11 00:00:00.0\t72132.78999999998\n2011-08-12 00:00:00.0\t10049.480000000005\n2011-08-14 00:00:00.0\t5150.18\n2011-08-15 00:00:00.0\t17205.54\n2011-08-16 00:00:00.0\t19103.710000000003\n2011-08-17 00:00:00.0\t49392.22000000001\n2011-08-18 00:00:00.0\t53225.67\n2011-08-19 00:00:00.0\t17248.54\n2011-08-21 00:00:00.0\t14549.210000000001\n2011-08-22 00:00:00.0\t27978.41\n2011-08-23 00:00:00.0\t25756.299999999992\n2011-08-24 00:00:00.0\t37074.89999999998\n2011-08-25 00:00:00.0\t22458.879999999997\n2011-08-26 00:00:00.0\t25550.230000000003\n2011-08-28 00:00:00.0\t10784.78\n2011-08-30 00:00:00.0\t31640.900000000023\n2011-08-31 00:00:00.0\t16117.999999999996\n2011-09-01 00:00:00.0\t37296.6\n2011-09-02 00:00:00.0\t41745.070000000014\n2011-09-04 00:00:00.0\t17018.49000000001\n2011-09-05 00:00:00.0\t36844.03999999999\n2011-09-06 00:00:00.0\t28052.62\n2011-09-07 00:00:00.0\t34125.65\n2011-09-08 00:00:00.0\t26708.000000000007\n2011-09-09 00:00:00.0\t29317.690000000002\n2011-09-11 00:00:00.0\t35465.470000000016\n2011-09-12 00:00:00.0\t29039.310000000005\n2011-09-13 00:00:00.0\t54828.450000000004\n2011-09-14 00:00:00.0\t23360.659999999996\n2011-09-15 00:00:00.0\t62943.81000000003\n2011-09-16 00:00:00.0\t25858.06\n2011-09-18 00:00:00.0\t15692.329999999998\n2011-09-19 00:00:00.0\t46212.21000000001\n2011-09-20 00:00:00.0\t109286.21\n2011-09-21 00:00:00.0\t42944.07000000001\n2011-09-22 00:00:00.0\t57076.829999999994\n2011-09-23 00:00:00.0\t39426.48\n2011-09-25 00:00:00.0\t31210.920999999995\n2011-09-26 00:00:00.0\t28642.271000000008\n2011-09-27 00:00:00.0\t35752.159999999996\n2011-09-28 00:00:00.0\t43383.03999999999\n2011-09-29 00:00:00.0\t43464.329999999994\n2011-09-30 00:00:00.0\t43992.850000000006\n2011-10-02 00:00:00.0\t11623.579999999998\n2011-10-03 00:00:00.0\t64214.78\n2011-10-04 00:00:00.0\t48240.840000000004\n2011-10-05 00:00:00.0\t75244.43000000002\n2011-10-06 00:00:00.0\t55306.28000000001\n2011-10-07 00:00:00.0\t47538.02000000001\n2011-10-09 00:00:00.0\t11922.240000000002\n2011-10-10 00:00:00.0\t44265.89000000002\n2011-10-11 00:00:00.0\t38267.75\n2011-10-12 00:00:00.0\t29302.850000000017\n2011-10-13 00:00:00.0\t37067.17\n2011-10-14 00:00:00.0\t35225.54\n2011-10-16 00:00:00.0\t21605.44\n2011-10-17 00:00:00.0\t47064.140000000014\n2011-10-18 00:00:00.0\t44637.84000000002\n2011-10-19 00:00:00.0\t36003.43000000001\n2011-10-20 00:00:00.0\t60793.139999999985\n2011-10-21 00:00:00.0\t62961.26000000002\n2011-10-23 00:00:00.0\t12302.410000000002\n2011-10-24 00:00:00.0\t38407.72000000003\n2011-10-25 00:00:00.0\t40807.49000000001\n2011-10-26 00:00:00.0\t37842.08\n2011-10-27 00:00:00.0\t47480.15000000001\n2011-10-28 00:00:00.0\t39559.47\n2011-10-30 00:00:00.0\t34545.279999999984\n2011-10-31 00:00:00.0\t48475.44999999995\n2011-11-01 00:00:00.0\t28741.550000000003\n2011-11-02 00:00:00.0\t45239.06000000002\n2011-11-03 00:00:00.0\t62816.550000000025\n2011-11-04 00:00:00.0\t60081.76\n2011-11-06 00:00:00.0\t42912.399999999994\n2011-11-07 00:00:00.0\t70001.07999999993\n2011-11-08 00:00:00.0\t56647.66000000001\n2011-11-09 00:00:00.0\t62599.42999999999\n2011-11-10 00:00:00.0\t68956.24000000003\n2011-11-11 00:00:00.0\t54835.51000000002\n2011-11-13 00:00:00.0\t33520.22000000001\n2011-11-14 00:00:00.0\t112141.10999999997\n2011-11-15 00:00:00.0\t60594.23000000001\n2011-11-16 00:00:00.0\t64408.7\n2011-11-17 00:00:00.0\t60329.72000000001\n2011-11-18 00:00:00.0\t48031.8\n2011-11-20 00:00:00.0\t34902.01\n2011-11-21 00:00:00.0\t48302.50000000003\n2011-11-22 00:00:00.0\t62307.31999999999\n2011-11-23 00:00:00.0\t78480.69999999997\n2011-11-24 00:00:00.0\t48080.28000000001\n2011-11-25 00:00:00.0\t50442.720000000016\n2011-11-27 00:00:00.0\t20571.500000000004\n2011-11-28 00:00:00.0\t55442.02000000003\n2011-11-29 00:00:00.0\t72219.2\n2011-11-30 00:00:00.0\t59150.98000000005\n2011-12-01 00:00:00.0\t51410.94999999999\n2011-12-02 00:00:00.0\t57086.060000000005\n2011-12-04 00:00:00.0\t24565.780000000002\n2011-12-05 00:00:00.0\t57751.32000000008\n2011-12-06 00:00:00.0\t54228.370000000024\n2011-12-07 00:00:00.0\t75076.22000000003\n2011-12-08 00:00:00.0\t81417.78\n2011-12-09 00:00:00.0\t32131.530000000006\n"},{"type":"TEXT","data":""}]},"apps":[],"jobName":"paragraph_1587490820281_1846673465","id":"20190520-140933_785400989","dateCreated":"2020-04-21T17:40:20+0000","dateStarted":"2020-04-22T16:59:10+0000","dateFinished":"2020-04-22T16:59:17+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:7917"},{"text":"//Weekly\n//write you DF solution here\nval weeklySalesDf = receiptAmountDf.groupBy(window(col(\"InvoiceDate\"), \"1 week\", \"1 week\", \"4 days\").getField(\"start\").alias(\"start\")).agg(sum('Amount).as(\"weeklySum\")).orderBy('start)\nweeklySalesDf.show(5)\nweeklySalesDf.createOrReplaceTempView(\"weeklySales\")\n//please verify using SparkSQL\nspark.sql(\"SELECT date_trunc('week', InvoiceDate) as start, sum(Amount) as weeklySum FROM invoiceView GROUP BY start ORDER BY start ASC LIMIT 5\").show\n","user":"anonymous","dateUpdated":"2020-04-24T14:38:56+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":6,"editorMode":"ace/mode/scala","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"+-------------------+------------------+\n|              start|         weeklySum|\n+-------------------+------------------+\n|2010-11-29 00:00:00|181847.25000000003|\n|2010-12-06 00:00:00| 270287.0299999999|\n|2010-12-13 00:00:00|207052.16999999995|\n|2010-12-20 00:00:00| 89770.56999999999|\n|2011-01-03 00:00:00| 93720.93000000001|\n+-------------------+------------------+\nonly showing top 5 rows\n\n+-------------------+------------------+\n|              start|         weeklySum|\n+-------------------+------------------+\n|2010-11-29 00:00:00|181847.25000000006|\n|2010-12-06 00:00:00| 270287.0299999999|\n|2010-12-13 00:00:00|207052.16999999998|\n|2010-12-20 00:00:00| 89770.56999999998|\n|2011-01-03 00:00:00|          93720.93|\n+-------------------+------------------+\n\nweeklySalesDf: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [start: timestamp, weeklySum: double]\n"}]},"apps":[],"jobName":"paragraph_1587490820282_2133491038","id":"20190520-140933_428817963","dateCreated":"2020-04-21T17:40:20+0000","dateStarted":"2020-04-22T17:13:36+0000","dateFinished":"2020-04-22T17:13:54+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:7918"},{"text":"%sql\n-- Plot weekly diagram here\nSELECT * from weeklySales","user":"anonymous","dateUpdated":"2020-04-24T14:41:53+0000","config":{"editorSetting":{"language":"sql","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":6,"editorMode":"ace/mode/sql","fontSize":9,"results":{"0":{"graph":{"mode":"lineChart","height":300,"optionOpen":false,"setting":{"table":{"tableGridState":{},"tableColumnTypeState":{"names":{"start":"string","weeklySum":"string"},"updated":false},"tableOptionSpecHash":"[{\"name\":\"useFilter\",\"valueType\":\"boolean\",\"defaultValue\":false,\"widget\":\"checkbox\",\"description\":\"Enable filter for columns\"},{\"name\":\"showPagination\",\"valueType\":\"boolean\",\"defaultValue\":false,\"widget\":\"checkbox\",\"description\":\"Enable pagination for better navigation\"},{\"name\":\"showAggregationFooter\",\"valueType\":\"boolean\",\"defaultValue\":false,\"widget\":\"checkbox\",\"description\":\"Enable a footer for displaying aggregated values\"}]","tableOptionValue":{"useFilter":false,"showPagination":false,"showAggregationFooter":false},"updated":false,"initialized":false},"multiBarChart":{"rotate":{"degree":"-45"},"xLabelStatus":"default"},"lineChart":{"rotate":{"degree":"-45"},"xLabelStatus":"default"}},"commonSetting":{},"keys":[{"name":"start","index":0,"aggr":"sum"}],"groups":[],"values":[{"name":"weeklySum","index":1,"aggr":"sum"}]},"helium":{}}},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TABLE","data":"start\tweeklySum\n2010-11-29 00:00:00.0\t181847.25\n2010-12-06 00:00:00.0\t270287.03000000014\n2010-12-13 00:00:00.0\t207052.1699999999\n2010-12-20 00:00:00.0\t89770.57\n2011-01-03 00:00:00.0\t93720.93000000004\n2011-01-10 00:00:00.0\t190994.96000000017\n2011-01-17 00:00:00.0\t133782.91000000003\n2011-01-24 00:00:00.0\t119136.81000000003\n2011-01-31 00:00:00.0\t123642.17999999995\n2011-02-07 00:00:00.0\t102296.93000000005\n2011-02-14 00:00:00.0\t139664.57999999996\n2011-02-21 00:00:00.0\t133069.93000000002\n2011-02-28 00:00:00.0\t130435.56000000001\n2011-03-07 00:00:00.0\t129180.95999999996\n2011-03-14 00:00:00.0\t145262.61000000004\n2011-03-21 00:00:00.0\t147909.80000000008\n2011-03-28 00:00:00.0\t183501.70999999988\n2011-04-04 00:00:00.0\t121598.22000000002\n2011-04-11 00:00:00.0\t147459.77099999998\n2011-04-18 00:00:00.0\t115461.24999999997\n2011-04-25 00:00:00.0\t84382.66000000006\n2011-05-02 00:00:00.0\t130354.3100000001\n2011-05-09 00:00:00.0\t205445.22999999995\n2011-05-16 00:00:00.0\t198031.53000000003\n2011-05-23 00:00:00.0\t160569.82\n2011-05-30 00:00:00.0\t116932.52000000014\n2011-06-06 00:00:00.0\t177916.87\n2011-06-13 00:00:00.0\t184016.49999999994\n2011-06-20 00:00:00.0\t117086.70999999999\n2011-06-27 00:00:00.0\t136287.44000000006\n2011-07-04 00:00:00.0\t174659.17000000007\n2011-07-11 00:00:00.0\t124163.1700000001\n2011-07-18 00:00:00.0\t182513.95999999993\n2011-07-25 00:00:00.0\t180814.85099999994\n2011-08-01 00:00:00.0\t153176.2\n2011-08-08 00:00:00.0\t161417.02000000008\n2011-08-15 00:00:00.0\t170724.89000000004\n2011-08-22 00:00:00.0\t149603.50000000006\n2011-08-29 00:00:00.0\t143819.06000000008\n2011-09-05 00:00:00.0\t190513.46999999986\n2011-09-12 00:00:00.0\t211722.61999999994\n2011-09-19 00:00:00.0\t326156.7210000001\n2011-09-26 00:00:00.0\t206858.23100000006\n2011-10-03 00:00:00.0\t302466.58999999997\n2011-10-10 00:00:00.0\t205734.63999999998\n2011-10-17 00:00:00.0\t263762.2199999999\n2011-10-24 00:00:00.0\t238642.19000000012\n2011-10-31 00:00:00.0\t288266.7699999998\n2011-11-07 00:00:00.0\t346560.14000000013\n2011-11-14 00:00:00.0\t380407.5699999999\n2011-11-21 00:00:00.0\t308185.0200000002\n2011-11-28 00:00:00.0\t319874.9900000002\n2011-12-05 00:00:00.0\t300605.22000000015\n"},{"type":"TEXT","data":""}]},"apps":[],"jobName":"paragraph_1587490820283_-1254657923","id":"20190520-212256_1274740776","dateCreated":"2020-04-21T17:40:20+0000","dateStarted":"2020-04-22T17:06:04+0000","dateFinished":"2020-04-22T17:06:17+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:7919"}],"name":"2-DataFrame_Aayushi","id":"2F9DNE9SY","noteParams":{},"noteForms":{},"angularObjects":{"md:shared_process":[],"spark:shared_process":[]},"config":{"isZeppelinNotebookCronEnable":false,"looknfeel":"default","personalizedMode":"false"},"info":{}}